---
title: "Chen_DataMining_HW2"
author: "Xuejun Chen"
date: "2018/2/20"
output:
  html_document:
    df_print: paged
---

# 6.a)
##the second score vs the first score
```{r}
# Load data
load("/Users/AudreyChen/Desktop/Data\ Mining/hw1.RData")

three.mean <- apply(train3, 2, mean)
three.sd <- apply(train3, 2, sd)
three.svd <- svd(train3)

three.score <- train3 %*% three.svd$v
plot(-three.score[,1],-three.score[,2])
```

## image of the mean of 3???s
```{r}
image(matrix(three.mean,16,16)[,16:1],col = grey(seq(0, 1, length = 256)))
```


## image of the first loading vector
```{r}
image(matrix(three.svd$v[,1],16,16)[,16:1],col = grey(seq(0, 1, length = 256)))
```


## image of the second loading vector
```{r}
image(matrix(three.svd$v[,2],16,16)[,16:1],col = grey(seq(0, 1, length = 256)))
```
## The first principal component has the largest possible variance, and the second component has the highest variance possible under the constraint that it is orthogonal to the preceding components. So the image of first component of 3 is more clear.

# 6.b) Repeat with digit 2???s
```{r}
two.mean <- apply(train2, 2, mean)
two.sd <- apply(train2, 2, sd)
two.svd <- svd(train2)

two.score <- train2 %*% two.svd$v
plot(-two.score[,1],-two.score[,2])
```

## image of the mean of 2???s
```{r}
image(matrix(two.mean,16,16)[,16:1],col = grey(seq(0, 1, length = 256)))
```

## image of the first loading vector
```{r}
image(matrix(two.svd$v[,1],16,16)[,16:1],col = grey(seq(0, 1, length = 256)))
```

## image of the second loading vector
```{r}
image(matrix(two.svd$v[,2],16,16)[,16:1],col = grey(seq(0, 1, length = 256)))
```


# 6.c)
```{r}
# Combine the training data for both digit 2 and 3
xx.train <- rbind(train2,train3)
xx.test <- rbind(test2,test3)

comb.mean <- apply(xx.train, 2, mean)
comb.sd <- apply(xx.train, 2, sd)
comb.svd <- svd(xx.train)

comb.score <- xx.train %*% comb.svd$v
plot(-comb.score[1:731,1],col="red",main = "Score 1 (Red:2,Blue:3)",ylim = c(2.5,14))
points(-comb.score[732:1389,1],col="blue")

plot(-comb.score[1:731,2],col="red",main = "Score 2 (Red:2,Blue:3)",ylim = c(-7,12))
points(-comb.score[732:1389,3],col="blue")

plot(-comb.score[1:731,1],-comb.score[1:731,2],col="red",lty=3,ylim = c(-10,10),xlim = c(1,13),xlab = "Score 1",ylab = "Score 2",main = "Score 1 vs. Score 2")
points(-comb.score[732:1389,1],-comb.score[732:1389,2],col="blue")
# Digit 3 has higher score 1, but lower score 2, comparing with digit 3. 
```

## image of the means
```{r}
image(matrix(comb.mean,16,16)[,16:1],col = grey(seq(0, 1, length = 256)))
```


## image of the first loading vector
```{r}
pr.out.c=prcomp(xx.train, scale=FALSE)
image(matrix(pr.out.c$rotation[,1],16,16)[,16:1],col = grey(seq(0, 1, length = 256)))
```

## image of the second loading vector
```{r}
image(matrix(pr.out.c$rotation[,2],16,16)[,16:1],col = grey(seq(0, 1, length = 256)))
```

## From the plot of the second score vs the first score, we can see the red dots are blue dots are not far from each other. Some red ones and blue ones are in the same region. Therefore, digit 2 and 3 share some similar properties. 


# 7.a)
```{r}
# Exclude V16 as mentioned in hw's instructions (because it has almost zero variance)
xx.train <- xx.train[,-16]
xx.test <- xx.test[,-16]

library(boot) # cv.glm function
y.train <- rep(0, 1389)
y.train[732:1389] <- 1
y.test <- rep(0,364)
y.test[199:364] <- 1

train.dat <- cbind(xx.train,y.train)
test.dat <- cbind(xx.test,y.test)
train.dat <- as.data.frame(train.dat)
test.dat <- as.data.frame(test.dat)

set.seed(1)
glm.fit <- glm(y.train~., data = train.dat)
cv.err <- cv.glm(train.dat, glm.fit,K=5)
cv.err$delta[1]
# The first component is the CV error without adjustment.
```

# 7.b)
```{r}
cost = function(y, y.hat) mean((y.hat>.5)!=y)
# repeat part a
set.seed(1)
cv.err2 <- cv.glm(train.dat, glm.fit,cost,K=5)
cv.err2$delta[1]
```

# 7.c)
```{r}
y.hat <- predict (glm.fit)
y.test.hat <- cbind(1, xx.test) %*% coef(glm.fit)
mean((y.test.hat >0.5)!=y.test)

```


# 8.a)
```{r}
# Two functions from given code
knn <- function(klist,x.train,y.train,x.test) {
  n.train <- nrow(x.train)
  n.test <- nrow(x.test)
  p.test <- matrix(NA, n.test, length(klist))
  dsq <- numeric(n.train)
  for (tst in 1:n.test)
  {
    for (trn in 1:n.train)
    {dsq[trn] <- sum((x.train[trn,] - x.test[tst,])^2)}
    
    ord <- order(dsq)
    
    for (ik in 1:length(klist)) {
      p.test[tst,ik] <- mean(y.train[ord[1:klist[ik]]])
    }}
  invisible(p.test)
}

knn.cv <- function(klist,x.train,y.train,nfolds) {
  n.train <- nrow(x.train)
  p.cv <- matrix(NA, n.train, length(klist))
  s <- split(sample(n.train),rep(1:nfolds,length=n.train))
  
  for (i in seq(nfolds)) {
    p.cv[s[[i]],] <- knn(klist,x.train[-s[[i]],], y.train[-s[[i]]], x.train[s[[i]],])
  }
  invisible(p.cv)
}
```

```{r}
klist <- seq(1,21,by=2)
# 5-fold CV
nfolds <- 5 
y.pred.train <- knn(klist,xx.train,y.train,xx.train)
y.pred.test <- knn(klist,xx.train,y.train,xx.test)
set.seed(1)
y.pred.cv <- knn.cv(klist,xx.train,y.train,nfolds)

# Compute errors
e.train = apply((y.pred.train> 0.5) != y.train,2,mean)
e.test = apply((y.pred.test> 0.5) != y.test,2,mean)
e.cv = apply((y.pred.cv> 0.5) != y.train,2,mean)

# Compare 
data.frame(klist,e.test, e.train,e.cv)

# Plot Misclassification errors as a function of k
plot(e.train , ylim=c(0,0.05) , type='l' , xlab='k' , ylab='Misclassification Error', col=1 , lwd=2)
lines(e.test , col=2 , lwd=2)
lines(e.cv, col=3 , lwd=2)
legend("topleft",legend=c('Train','Test','CV'),text.col=seq(3) , lty=1 , col=seq(3))

# 8.b)
# best K
which.min(e.cv)

# CV error and test error at K = 1
e.test[which.min(e.cv)]
e.cv[which.min(e.cv)]

# Blue circles the CV error and test error at K =1
points(1, e.test[which.min(e.cv)], col = 4)
points(1, e.cv[which.min(e.cv)], col = 4)
```

# 9.a)
```{r}
library(glmnet)

set.seed(1)
# Fit ridge regression model on training data
ridge.mod <- glmnet(xx.train, y.train, alpha=0, standardize = FALSE)  
cv.ridge <- cv.glmnet(xx.train, y.train, alpha = 0, standardize = FALSE,nfolds = 5) 

# plot using the built-in plot function
plot(cv.ridge)
#plot(log(cv.ridge$lambda),cv.ridge$cvm)

# Select ?? that minimizes training MSE
best.lam1 <- cv.ridge$lambda.min 
best.lam1

ridge.test1 <- predict(ridge.mod, s = best.lam1,newx=xx.test)
res1 <- ridge.test1 > 0.5
mean(res1 != y.test)

# best ?? according to One-Standard Error Rule
best.lam2 <- cv.ridge$lambda.1se
best.lam2

ridge.test2 <- predict(ridge.mod, s = best.lam2,newx=xx.test)
res2 <- ridge.test2 > 0.5
mean(res2 != y.test)
```

# 9.b)
```{r}
pot.lambda <- cv.ridge$lambda

k = 5
set.seed(1)
nfolds = sample(1:k,length(pot.lambda),replace=TRUE)
table(nfolds)

cv.ridge.err = NA
newlambda = NA

for (i in 1:5){
  cv4ridge = glmnet(xx.train, y.train, alpha=0, lambda=pot.lambda[nfolds != i], standardize = FALSE)
  ridge.pred <- predict(cv4ridge,s=pot.lambda[nfolds == i],newx=xx.train)
  err <- apply((ridge.pred> 0.5)!= y.train,2,mean)
  cv.ridge.err <- append(cv.ridge.err,err)
  newlambda <- append(newlambda,pot.lambda[nfolds == i])
  }

length(cv.ridge.err)
length(newlambda)

cv.ridge.err=as.vector(na.omit(cv.ridge.err))
newlambda=as.vector(na.omit(newlambda))

# plot
plot(log(newlambda),cv.ridge.err)

# Smallest CV misclassification error
which.min(cv.ridge.err)
min(cv.ridge.err)
# ?? that has the smallest CV misclassification error
newlambda[which.min(cv.ridge.err)]
```

# 10.a)
```{r}
# Fit Lasso regression model on training data
lasso.mod <- glmnet(xx.train,y.train,alpha=1,standardize = FALSE) 
cv.lasso <- cv.glmnet(xx.train, y.train, alpha = 1, standardize = FALSE,nfolds = 5) 

# plot
plot(log(cv.lasso$lambda),cv.lasso$cvm)

# Select lamda that minimizes training MSE
las.best.lam1 <- cv.lasso$lambda.min 
las.best.lam1

lasso.test1 <- predict(lasso.mod, s = las.best.lam1,newx=xx.test)
res1 <- lasso.test1 > 0.5
mean(res1 != y.test)

# best ?? according to One-Standard Error Rule
las.best.lam2 <- cv.lasso$lambda.1se
las.best.lam2

lasso.test2 <- predict(lasso.mod, s = las.best.lam2,newx=xx.test)
res2 <- lasso.test2 > 0.5
mean(res2 != y.test)
```

# 10.b)
```{r}
pot.lambda.lasso <- cv.lasso$lambda

k = 5
set.seed(1)
nfolds = sample(1:k,length(pot.lambda.lasso),replace=TRUE)
table(nfolds)

cv.lasso.err = NA
lambda.lasso = NA

for (i in 1:5){
  cv4lasso = glmnet(xx.train, y.train, alpha=1, lambda=pot.lambda.lasso[nfolds != i], standardize = FALSE)
  lasso.pred <- predict(cv4lasso,s=pot.lambda.lasso[nfolds == i],newx=xx.train)
  err.l <- apply((lasso.pred> 0.5)!= y.train,2,mean)
  cv.lasso.err <- append(cv.lasso.err,err.l)
  lambda.lasso <- append(lambda.lasso,pot.lambda[nfolds == i])
}

cv.lasso.err=na.omit(cv.lasso.err)
lambda.lasso=na.omit(lambda.lasso)

# plot
plot(log(lambda.lasso),cv.lasso.err)

# Smallest CV misclassification error
which.min(cv.lasso.err)
min(cv.lasso.err)
# ?? that has the smallest CV misclassification error
lambda.lasso[which.min(cv.lasso.err)]
```

# 11
```{r}
library(pls)
```

```{r,warning=FALSE}
## PCR
k = 5

# the following ideas from https://stats.stackexchange.com/questions/46216/pca-and-k-fold-cross-validation-in-caret-package-in-r
# pick the number of components
# for each fold:
#    split data
#    fit linear regression on the 90% used for training
#    predict the 10% held out
# end:

k = 5
set.seed(1)
nfolds = sample(1:k,nrow(train.dat),replace=TRUE)
table(nfolds)

cv.err.pcr = matrix(NA,5,255)
for (i in 1:5){
  pcr.fit=pcr(y.train~., data=train.dat[nfolds != i,],scale=FALSE, validation="none")
  for (j in 1:255){
  pcr.pred <- predict(pcr.fit,data=train.dat[nfolds == i,], ncomp = j)
  cv.err.pcr[i,j] = mean((pcr.pred> 0.5)!= y.train[nfolds == i])
  }
}

mean.cv.err.pcr = apply(cv.err.pcr,2,mean)

str(mean.cv.err.pcr)
plot(mean.cv.err.pcr, pch = 19, type = "b",ylab = "Errors")
plot(mean.cv.err.pcr[1:40], pch = 19, type = "b",ylab = "Errors")
# According to the plot, 12 components.

pcr.fit2=pcr(y.train~., data=as.data.frame(train.dat),scale=FALSE, validation="none")
pcr.pred.train <- predict(pcr.fit2,data=train.dat, ncomp = 12)
pcr.pred.test <- predict(pcr.fit2,data=test.dat, ncomp = 12)

# Compute errors
pcr.e.train = mean((pcr.pred.train> 0.5) != y.train)
pcr.e.train
pcr.e.test = mean((pcr.pred.test> 0.5) != y.test)
pcr.e.test
```

# 12
```{r}
library(leaps)

# Some functions are from https://rpubs.com/davoodastaraky/subset
predict.regsubsets =function (object ,newdata ,id ,...){
  form=as.formula(object$call [[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object ,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}

k = 5
set.seed(1)
folds = sample(1:k,nrow(train.dat),replace=TRUE)
table(folds)

## There are some erros in a outer loop of k, so the code is seperated for five parts. 
# 1
cv.errors=matrix(NA,k,254)

train.dat = as.data.frame(train.dat)
best.fit1 = regsubsets(y.train ~., data=train.dat[folds != 1,], nvmax = 255,method="forward")
for (i in 1:254){
  pred = predict.regsubsets(best.fit1, train.dat[folds == 1, ], id = i)
  cv.errors[1,i] = mean((pred> 0.5)!= y.train[folds == 1])
}

# 2
best.fit2 = regsubsets(y.train ~., data=train.dat[folds != 2,], nvmax = 255,method="forward")
for (i in 1:254){
  pred = predict.regsubsets(best.fit2, train.dat[folds == 2, ], id = i)
  cv.errors[2,i] = mean((pred> 0.5)!= y.train[folds == 2])
}

# 3
best.fit3 = regsubsets(y.train ~., data=train.dat[folds != 3,], nvmax = 255,method="forward")
for (i in 1:254){
  pred = predict.regsubsets(best.fit3, train.dat[folds == 3, ], id = i)
  cv.errors[3,i] = mean((pred> 0.5)!= y.train[folds == 3])
}

# 4
best.fit4 = regsubsets(y.train ~., data=train.dat[folds != 4,], nvmax = 255,method="forward")
for (i in 1:254){
  pred = predict.regsubsets(best.fit4, train.dat[folds == 4, ], id = i)
  cv.errors[4,i] = mean((pred> 0.5)!= y.train[folds == 4])
}

# 5
best.fit5 = regsubsets(y.train ~., data=train.dat[folds != 5,], nvmax = 255,method="forward")
for (i in 1:254){
  pred = predict.regsubsets(best.fit4, train.dat[folds == 5, ], id = i)
  cv.errors[5,i] = mean((pred> 0.5)!= y.train[folds == 5])
}

mean.cv.errors = apply(cv.errors,2,mean)
length(mean.cv.errors)
plot(mean.cv.errors, pch = 19, type = "b",ylab = "Errors")
# There are too many predictors. So I picked first 30 which seem to have small variables to take
# zoom in the plot
plot(mean.cv.errors[0:30], pch = 19, type = "b",ylab = "Errors")

# Compare 18-variable and 24-variables model
mean.cv.errors[18]
mean.cv.errors[24]
# 24-variable model seems to be good enough.

reg.best <- regsubsets(y.train ~., data=train.dat,nvmax = 255,method="forward")
best.coef <- coef(reg.best ,24)
yhat.train <- (cbind(1, xx.train[,names(best.coef[-1])]) %*% best.coef) > .5
best.train.errors <- mean(yhat.train != y.train)
best.train.errors
yhat.test <- (cbind(1, xx.test[,names(best.coef[-1])]) %*% best.coef) > .5
best.test.errors <- mean(yhat.test != y.test)
best.test.errors
```

